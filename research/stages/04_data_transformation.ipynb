{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\anfe1\\\\OneDrive\\\\Escritorio\\\\Instaleap\\\\Instamarket'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    stores_list_file: Path\n",
    "    preprocessor_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from instamarket.constants import CONFIG_FILE_PATH\n",
    "from instamarket.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self) -> None:\n",
    "        config_file_path = CONFIG_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(config_file_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            stores_list_file=config.stores_list_file,\n",
    "            preprocessor_file=config.preprocessor_file\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from instamarket.utils.common import save_object, load_object\n",
    "from instamarket.logging import logger\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config:DataTransformationConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def get_data_transformer_obj(self):\n",
    "        \"\"\"\n",
    "        This function is responsible for data transformation \n",
    "\n",
    "        \"\"\"\n",
    "        numerical_columns = [\"optimal_total_time\"]\n",
    "        categorical_columns = [\n",
    "            \"store_id\",\n",
    "            \"optimal_start_day\", \"optimal_start_hour\", \"optimal_start_minute\", \"optimal_start_weekday\",\n",
    "            \"optimal_end_day\", \"optimal_end_hour\", \"optimal_end_minute\", \"optimal_end_weekday\"\n",
    "            ]\n",
    "\n",
    "        num_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"scaler\",StandardScaler())\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"Numerical columns standard scaling completed\")\n",
    "\n",
    "        stores = load_object(self.config.stores_list_file)\n",
    "        datetime_categories = [list(range(1,32)),list(range(0,24)),list(range(0,60)),list(range(0,7))]\n",
    "        categories = [stores,*datetime_categories,*datetime_categories]\n",
    "        cat_pipeline = Pipeline(\n",
    "            steps=[\n",
    "                (\"one_hot_encoder\",OneHotEncoder(categories=categories)),\n",
    "                (\"scaler\",StandardScaler(with_mean=False))\n",
    "            ]\n",
    "        )\n",
    "        logger.info(\"Categorical columns encoding completed\")\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            [\n",
    "                (\"num_pipeline\",num_pipeline,numerical_columns),\n",
    "                (\"cat_pipeline\",cat_pipeline,categorical_columns)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logger.info(\"Save preprocessing object\")\n",
    "        save_object(self.config.preprocessor_file, preprocessor)\n",
    "\n",
    "        return preprocessor\n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self, preprocessing_obj, target_column_name):\n",
    "        logger.info(\"Read train & test data as dataframe\")\n",
    "        train_df = pd.read_csv(os.path.join(self.config.data_path,\"train.csv\"))\n",
    "        test_df = pd.read_csv(os.path.join(self.config.data_path,\"test.csv\"))\n",
    "\n",
    "        input_feature_train_df = train_df.drop(columns=target_column_name,axis=1)\n",
    "        target_feature_train_df = train_df[target_column_name]\n",
    "\n",
    "        input_feature_test_df = test_df.drop(columns=target_column_name,axis=1)\n",
    "        target_feature_test_df = test_df[target_column_name]\n",
    "\n",
    "        logger.info(\"Applying preprocessing object on training and testing dataframes\")\n",
    "        input_feature_train_arr = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "        input_feature_test_arr = preprocessing_obj.fit_transform(input_feature_test_df)\n",
    "\n",
    "        train_arr = hstack([input_feature_train_arr,np.array(target_feature_train_df)])\n",
    "        test_arr = hstack([input_feature_test_arr,np.array(target_feature_test_df)])\n",
    "\n",
    "        save_object(os.path.join(self.config.root_dir,\"train.pkl\"), train_arr)\n",
    "        save_object(os.path.join(self.config.root_dir,\"test.pkl\"), test_arr)\n",
    "\n",
    "    def convert(self):\n",
    "        preprocessing_obj = self.get_data_transformer_obj()\n",
    "        self.initiate_data_transformation(preprocessing_obj, [\"start_delay\", \"end_delay\"])\n",
    "        logger.info(\"Transformed dataset saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:10:07,830] 29 common - INFO - yaml file config\\config.yml loaded successfully\n",
      "[2024-04-20 15:10:07,830] 47 common - INFO - Created directory at: artifacts\n",
      "[2024-04-20 15:10:07,839] 47 common - INFO - Created directory at: artifacts/data_transformation\n",
      "[2024-04-20 15:10:07,840] 34 232841500 - INFO - Numerical columns standard scaling completed\n",
      "[2024-04-20 15:10:07,842] 45 232841500 - INFO - Categorical columns encoding completed\n",
      "[2024-04-20 15:10:07,844] 54 232841500 - INFO - Save preprocessing object\n",
      "[2024-04-20 15:10:07,851] 61 232841500 - INFO - Read train & test data as dataframe\n",
      "[2024-04-20 15:10:08,376] 71 232841500 - INFO - Applying preprocessing object on training and testing dataframes\n",
      "[2024-04-20 15:10:09,146] 84 232841500 - INFO - Transformed dataset saved\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
